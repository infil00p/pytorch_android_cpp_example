{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MobileNetV2(\n",
       "  (features): Sequential(\n",
       "    (0): ConvBNActivation(\n",
       "      (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU6(inplace=True)\n",
       "    )\n",
       "    (1): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): ConvBNActivation(\n",
       "          (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "          (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): Conv2d(32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (2): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): ConvBNActivation(\n",
       "          (0): Conv2d(16, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): ConvBNActivation(\n",
       "          (0): Conv2d(96, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=96, bias=False)\n",
       "          (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (2): Conv2d(96, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (3): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): ConvBNActivation(\n",
       "          (0): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): ConvBNActivation(\n",
       "          (0): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=144, bias=False)\n",
       "          (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (2): Conv2d(144, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (4): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): ConvBNActivation(\n",
       "          (0): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): ConvBNActivation(\n",
       "          (0): Conv2d(144, 144, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=144, bias=False)\n",
       "          (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (2): Conv2d(144, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (5): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): ConvBNActivation(\n",
       "          (0): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): ConvBNActivation(\n",
       "          (0): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)\n",
       "          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (2): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (6): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): ConvBNActivation(\n",
       "          (0): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): ConvBNActivation(\n",
       "          (0): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)\n",
       "          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (2): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (7): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): ConvBNActivation(\n",
       "          (0): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): ConvBNActivation(\n",
       "          (0): Conv2d(192, 192, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=192, bias=False)\n",
       "          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (2): Conv2d(192, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (8): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): ConvBNActivation(\n",
       "          (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): ConvBNActivation(\n",
       "          (0): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n",
       "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (2): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (9): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): ConvBNActivation(\n",
       "          (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): ConvBNActivation(\n",
       "          (0): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n",
       "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (2): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (10): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): ConvBNActivation(\n",
       "          (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): ConvBNActivation(\n",
       "          (0): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n",
       "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (2): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (11): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): ConvBNActivation(\n",
       "          (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): ConvBNActivation(\n",
       "          (0): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n",
       "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (2): Conv2d(384, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (12): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): ConvBNActivation(\n",
       "          (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): ConvBNActivation(\n",
       "          (0): Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576, bias=False)\n",
       "          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (2): Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (13): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): ConvBNActivation(\n",
       "          (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): ConvBNActivation(\n",
       "          (0): Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576, bias=False)\n",
       "          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (2): Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (14): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): ConvBNActivation(\n",
       "          (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): ConvBNActivation(\n",
       "          (0): Conv2d(576, 576, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=576, bias=False)\n",
       "          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (2): Conv2d(576, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (15): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): ConvBNActivation(\n",
       "          (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): ConvBNActivation(\n",
       "          (0): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False)\n",
       "          (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (2): Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (16): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): ConvBNActivation(\n",
       "          (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): ConvBNActivation(\n",
       "          (0): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False)\n",
       "          (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (2): Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (17): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): ConvBNActivation(\n",
       "          (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): ConvBNActivation(\n",
       "          (0): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False)\n",
       "          (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (2): Conv2d(960, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (18): ConvBNActivation(\n",
       "      (0): Conv2d(320, 1280, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (1): BatchNorm2d(1280, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU6(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (classifier): Sequential(\n",
       "    (0): Dropout(p=0.2, inplace=False)\n",
       "    (1): Linear(in_features=1280, out_features=1000, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import numpy as np\n",
    "import cv2 as cv\n",
    "from torch.utils.mobile_optimizer import optimize_for_mobile\n",
    "\n",
    "model = torchvision.models.mobilenet_v2(pretrained=True)\n",
    "\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download an example image from the pytorch website\n",
    "import urllib\n",
    "url, filename = (\"https://github.com/pytorch/hub/raw/master/images/dog.jpg\", \"dog.jpg\")\n",
    "try: urllib.URLopener().retrieve(url, filename)\n",
    "except: urllib.request.urlretrieve(url, filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "m_mean=[0.485, 0.456, 0.406]\n",
    "m_std=[0.229, 0.224, 0.225]\n",
    "opencv_input_image = cv.imread(filename)\n",
    "res = cv.resize(opencv_input_image,(224, 224), interpolation = cv.INTER_CUBIC)\n",
    "im_rgb = cv.cvtColor(res, cv.COLOR_BGR2RGB)\n",
    "im_rgb = np.float32(im_rgb)/255\n",
    "r,g,b = cv.split(im_rgb)\n",
    "r = (r - m_mean[0])/m_std[0]\n",
    "g = (g - m_mean[1])/m_std[1]\n",
    "b = (b - m_mean[2])/m_std[2]\n",
    "pre_process = cv.merge((r,g,b))\n",
    "final_pre_process = cv.dnn.blobFromImage(pre_process)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample execution (requires torchvision)\n",
    "#from PIL import Image\n",
    "#from torchvision import transforms\n",
    "\n",
    "#input_image = Image.open(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[[-1.8610, -1.7754, -1.7583,  ..., -2.1008, -2.1008, -1.8439],\n",
       "           [-2.0323, -1.8953, -1.9638,  ..., -2.0837, -1.9638, -1.9295],\n",
       "           [-1.8782, -1.9809, -2.0665,  ..., -2.0494, -2.0323, -1.9809],\n",
       "           ...,\n",
       "           [-1.8097, -1.5357, -1.5528,  ..., -1.5357, -1.3130, -1.3302],\n",
       "           [-1.8268, -1.1418, -1.6213,  ..., -1.6727, -1.3644, -1.1760],\n",
       "           [-1.1589, -1.0390, -1.6384,  ..., -1.3473, -1.7412, -1.1932]],\n",
       "\n",
       "          [[-1.6331, -1.6155, -1.6681,  ..., -1.9657, -1.8782, -1.5280],\n",
       "           [-1.8606, -1.5980, -1.5280,  ..., -2.0357, -2.0007, -1.8256],\n",
       "           [-1.7731, -1.8431, -1.8957,  ..., -1.9307, -1.9482, -1.8606],\n",
       "           ...,\n",
       "           [-0.8978, -0.9503, -0.8277,  ..., -0.9503, -0.5826, -0.4601],\n",
       "           [-1.2304, -0.6702, -0.8102,  ..., -0.8277, -0.6176, -0.4076],\n",
       "           [-0.4076, -0.7052, -0.9678,  ..., -0.7402, -1.0553, -0.5301]],\n",
       "\n",
       "          [[-1.4733, -1.4384, -1.4210,  ..., -1.8044, -1.7870, -1.6302],\n",
       "           [-1.7522, -1.6999, -1.5256,  ..., -1.7870, -1.7522, -1.6302],\n",
       "           [-1.6476, -1.7522, -1.7173,  ..., -1.7347, -1.7173, -1.6824],\n",
       "           ...,\n",
       "           [-1.5779, -1.4559, -1.6650,  ..., -1.5953, -1.3164, -1.1770],\n",
       "           [-1.5430, -1.0376, -1.6302,  ..., -1.5430, -1.1944, -1.0027],\n",
       "           [-0.7238, -0.7064, -1.5430,  ..., -1.4559, -1.2816, -0.7064]]]]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#preprocess = transforms.Compose([\n",
    "#    transforms.Resize(224),\n",
    "#    transforms.ToTensor(),\n",
    "#    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "#])\n",
    "#input_tensor = preprocess(input_image)\n",
    "input_tensor = torch.from_numpy(final_pre_process)\n",
    "input_batch = input_tensor.unsqueeze(0) # create a mini-batch as expected by the model\n",
    "input_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([3.4588e-07, 1.6758e-08, 2.7807e-08, 1.8452e-08, 4.7866e-08, 5.0434e-07,\n",
       "        3.3854e-07, 6.0622e-05, 1.1578e-03, 2.9473e-07, 2.6800e-10, 3.6728e-08,\n",
       "        4.8528e-10, 1.1868e-08, 2.4164e-09, 7.1094e-09, 6.4921e-08, 1.7225e-06,\n",
       "        5.8309e-07, 1.0582e-08, 1.8381e-08, 6.1718e-09, 3.2813e-08, 1.0519e-07,\n",
       "        2.2854e-08, 3.8548e-08, 2.5787e-08, 2.9939e-07, 4.3841e-08, 3.8215e-07,\n",
       "        3.9192e-07, 2.0235e-07, 1.0700e-07, 2.9171e-09, 4.6196e-08, 5.4040e-09,\n",
       "        5.4424e-08, 1.5754e-08, 6.8187e-08, 2.0042e-06, 3.1404e-08, 9.0537e-09,\n",
       "        7.4664e-08, 1.3874e-08, 3.6577e-07, 2.1438e-07, 9.0882e-07, 9.4028e-09,\n",
       "        1.3414e-07, 3.6072e-07, 2.6114e-06, 8.7284e-08, 8.9276e-08, 6.8340e-08,\n",
       "        1.7829e-07, 1.5330e-08, 1.2911e-07, 1.2032e-08, 1.0370e-07, 2.1345e-08,\n",
       "        1.2658e-06, 4.7681e-09, 4.8863e-08, 5.0200e-10, 3.3416e-09, 1.1046e-08,\n",
       "        2.2036e-07, 3.8540e-08, 7.1286e-08, 2.0001e-08, 9.7509e-09, 1.7718e-07,\n",
       "        8.6142e-09, 7.5555e-09, 2.3858e-08, 2.3912e-08, 3.7059e-08, 3.2888e-08,\n",
       "        7.7016e-07, 1.2912e-06, 2.0248e-08, 5.0239e-07, 1.0500e-06, 1.1112e-06,\n",
       "        1.9576e-05, 4.7071e-07, 3.1827e-07, 7.4484e-08, 2.6671e-08, 5.1705e-05,\n",
       "        2.6019e-08, 5.7953e-09, 4.5540e-09, 1.5768e-08, 1.1100e-08, 2.7368e-10,\n",
       "        2.0644e-07, 1.2930e-08, 4.8475e-10, 1.2373e-06, 6.3006e-07, 1.5019e-09,\n",
       "        2.3638e-07, 7.3496e-09, 9.3916e-03, 4.2287e-07, 1.0867e-05, 2.4083e-07,\n",
       "        5.9518e-07, 1.1934e-08, 1.6594e-07, 4.3151e-09, 1.8677e-07, 1.0006e-06,\n",
       "        3.4768e-06, 6.5114e-07, 8.7181e-08, 3.1819e-08, 5.5613e-07, 4.0118e-07,\n",
       "        8.7451e-10, 3.2588e-07, 1.6302e-07, 9.0966e-07, 3.0464e-06, 1.8273e-08,\n",
       "        1.5560e-07, 2.0542e-07, 3.6142e-09, 7.0172e-07, 8.0378e-09, 2.4653e-08,\n",
       "        1.1285e-06, 1.4637e-09, 3.7899e-07, 1.4335e-08, 1.8332e-08, 4.7599e-09,\n",
       "        5.4112e-08, 2.7008e-10, 9.3663e-10, 8.5073e-09, 4.0884e-09, 9.1284e-09,\n",
       "        2.1605e-07, 1.3305e-06, 6.6148e-07, 5.8239e-09, 1.7857e-08, 6.1936e-08,\n",
       "        1.5177e-08, 5.2299e-04, 3.0764e-05, 1.7434e-04, 9.6025e-05, 6.5942e-06,\n",
       "        3.4924e-07, 3.5151e-04, 5.2992e-06, 3.7823e-07, 4.4061e-07, 1.5427e-07,\n",
       "        3.1670e-07, 4.9234e-08, 3.0582e-08, 6.1569e-09, 9.7921e-08, 8.8216e-09,\n",
       "        4.3278e-07, 1.5218e-05, 7.7547e-05, 3.0628e-06, 7.5358e-08, 1.0160e-04,\n",
       "        3.4136e-04, 1.0291e-06, 1.4072e-08, 5.1319e-06, 5.2992e-07, 6.1060e-07,\n",
       "        4.1389e-06, 5.6265e-07, 6.1476e-06, 1.0719e-06, 2.3368e-05, 1.4419e-04,\n",
       "        5.8529e-04, 9.4565e-07, 4.2788e-05, 3.6274e-06, 2.8182e-05, 4.0939e-07,\n",
       "        7.7479e-04, 2.7515e-04, 1.6749e-06, 5.4207e-06, 2.5932e-06, 2.7640e-06,\n",
       "        2.3125e-06, 2.3777e-03, 2.9237e-06, 4.9139e-06, 2.0118e-06, 3.8964e-02,\n",
       "        7.5962e-06, 1.1020e-06, 2.3458e-07, 2.6584e-05, 1.4199e-05, 9.9378e-07,\n",
       "        3.9849e-08, 3.2151e-06, 7.4593e-06, 5.7125e-07, 8.2522e-08, 5.1237e-06,\n",
       "        1.1917e-05, 3.9491e-06, 1.3756e-06, 1.5220e-05, 9.1876e-07, 3.3611e-07,\n",
       "        1.9377e-04, 7.5630e-03, 7.0728e-04, 9.7548e-06, 1.6086e-05, 1.3675e-04,\n",
       "        4.0072e-05, 1.9999e-04, 5.9511e-03, 4.1076e-03, 6.6592e-04, 2.5135e-06,\n",
       "        6.4200e-07, 3.9969e-05, 3.5803e-06, 1.2806e-05, 1.3688e-06, 2.9189e-06,\n",
       "        1.6110e-06, 8.5992e-07, 3.8063e-07, 4.3043e-08, 1.2086e-07, 6.1855e-07,\n",
       "        5.1866e-07, 2.5972e-06, 3.0533e-03, 1.3632e-03, 2.5050e-03, 3.4382e-06,\n",
       "        6.0442e-07, 8.4570e-06, 2.8983e-07, 2.2762e-07, 3.8448e-05, 5.2614e-03,\n",
       "        6.1352e-01, 2.4764e-01, 2.8910e-04, 2.8367e-02, 7.0085e-08, 7.1633e-04,\n",
       "        2.2227e-05, 2.9674e-05, 5.2385e-05, 4.5794e-05, 1.0057e-06, 3.0264e-05,\n",
       "        4.1437e-03, 7.7938e-07, 9.0683e-06, 1.2423e-05, 2.3463e-06, 1.9848e-08,\n",
       "        4.8619e-07, 7.4776e-05, 4.4040e-05, 9.5176e-03, 3.7890e-05, 1.8331e-05,\n",
       "        8.7283e-06, 9.2028e-04, 3.3087e-04, 2.8071e-05, 8.5568e-08, 5.6468e-05,\n",
       "        2.0912e-07, 1.2506e-06, 1.0068e-08, 4.1083e-08, 2.1815e-07, 9.9944e-08,\n",
       "        4.7995e-08, 7.8822e-08, 4.6934e-07, 5.9374e-09, 5.3364e-07, 1.7249e-07,\n",
       "        4.1689e-10, 2.8510e-08, 2.2348e-08, 7.3156e-08, 3.6403e-08, 1.8468e-08,\n",
       "        2.0620e-07, 2.1629e-07, 6.8920e-09, 5.1294e-08, 1.1253e-06, 2.4103e-07,\n",
       "        2.1233e-07, 1.2732e-06, 5.7855e-07, 1.0633e-08, 4.4505e-08, 1.0890e-07,\n",
       "        3.3261e-07, 1.0333e-09, 1.8212e-09, 1.0661e-08, 8.5956e-09, 8.9955e-09,\n",
       "        3.3173e-08, 2.1016e-07, 5.9054e-08, 2.3140e-07, 1.0081e-07, 1.4771e-06,\n",
       "        4.0755e-05, 7.8032e-04, 2.1629e-03, 7.2501e-05, 7.0938e-06, 1.1337e-05,\n",
       "        1.2605e-05, 4.7027e-07, 2.9803e-05, 8.4342e-08, 9.8009e-09, 6.3739e-06,\n",
       "        5.3190e-07, 1.5374e-08, 1.9261e-08, 1.6387e-07, 3.9317e-08, 6.0420e-08,\n",
       "        6.6254e-05, 2.5534e-07, 2.0401e-07, 1.1508e-09, 3.7711e-09, 2.7960e-07,\n",
       "        8.5450e-09, 2.8278e-04, 8.1171e-05, 1.9749e-06, 3.7429e-05, 1.3434e-04,\n",
       "        3.7030e-07, 4.5713e-06, 9.8947e-07, 1.9453e-06, 4.9782e-08, 3.0886e-08,\n",
       "        2.0294e-08, 4.6394e-09, 1.4828e-07, 6.5780e-09, 9.1422e-08, 3.9910e-07,\n",
       "        3.0020e-07, 4.8781e-06, 3.2279e-07, 2.0162e-08, 1.3642e-09, 2.5538e-06,\n",
       "        3.1528e-07, 2.4363e-08, 6.6099e-07, 2.4612e-07, 2.4258e-07, 3.1354e-07,\n",
       "        2.1043e-07, 6.9363e-10, 2.1253e-10, 7.6771e-08, 7.4070e-08, 6.5995e-08,\n",
       "        8.1410e-08, 8.8999e-07, 2.1720e-09, 2.4160e-07, 1.1413e-08, 1.0864e-06,\n",
       "        5.0749e-08, 1.7534e-07, 9.4957e-08, 3.9117e-08, 2.8506e-09, 1.3096e-07,\n",
       "        3.5561e-06, 8.9556e-08, 4.1686e-08, 3.0531e-07, 2.0999e-08, 1.9460e-07,\n",
       "        1.1210e-06, 1.7864e-07, 3.7629e-09, 3.8802e-06, 6.6175e-06, 5.8098e-07,\n",
       "        5.1556e-08, 3.9367e-07, 1.6453e-08, 2.7844e-07, 6.0171e-08, 4.1792e-07,\n",
       "        1.7345e-06, 6.3264e-07, 6.6522e-08, 1.9265e-08, 4.2478e-08, 2.5694e-06,\n",
       "        5.2307e-07, 5.5518e-06, 3.6302e-05, 1.4108e-06, 5.5076e-09, 2.9517e-07,\n",
       "        5.4051e-07, 5.1031e-08, 1.1496e-05, 2.3590e-06, 1.7910e-06, 4.4031e-06,\n",
       "        1.3740e-08, 7.9657e-08, 2.3676e-05, 8.4320e-07, 6.0824e-07, 6.6298e-07,\n",
       "        1.3728e-06, 5.0817e-07, 6.3191e-07, 2.0869e-06, 1.0816e-07, 2.1259e-07,\n",
       "        1.0318e-06, 1.6202e-08, 7.1163e-07, 1.6559e-07, 1.0681e-08, 3.1327e-07,\n",
       "        4.9423e-07, 3.9095e-06, 2.3738e-06, 2.1045e-07, 3.4557e-05, 1.2212e-06,\n",
       "        1.2253e-05, 1.3568e-05, 1.7816e-07, 1.2495e-08, 4.5366e-09, 8.6975e-09,\n",
       "        3.7425e-07, 6.2431e-07, 1.8978e-06, 3.7473e-06, 3.2393e-06, 7.1507e-08,\n",
       "        9.1099e-08, 2.7222e-08, 7.3638e-08, 1.9595e-07, 7.1825e-06, 4.7649e-07,\n",
       "        1.5025e-07, 2.8873e-08, 8.0239e-08, 1.0239e-05, 5.2914e-06, 2.7670e-07,\n",
       "        3.0429e-08, 2.3809e-07, 2.3746e-06, 5.7694e-05, 1.6945e-06, 9.2531e-07,\n",
       "        1.1828e-06, 9.3504e-07, 5.8272e-07, 5.7519e-07, 2.6195e-05, 1.6684e-06,\n",
       "        1.9488e-06, 3.2853e-07, 9.9031e-09, 3.3212e-06, 1.2782e-05, 1.0810e-06,\n",
       "        2.0878e-07, 9.4568e-07, 1.1269e-06, 3.6654e-07, 4.8289e-07, 2.4275e-08,\n",
       "        2.5038e-08, 9.6285e-07, 6.1180e-07, 2.2492e-06, 1.5271e-07, 2.7282e-06,\n",
       "        6.9635e-07, 3.1523e-08, 5.1758e-08, 3.1880e-06, 1.1723e-07, 1.2840e-07,\n",
       "        2.8011e-05, 2.5181e-06, 3.1831e-07, 3.7933e-07, 7.9372e-09, 3.5694e-08,\n",
       "        3.9637e-08, 5.8592e-07, 5.7830e-07, 1.8769e-08, 3.8251e-08, 6.3533e-07,\n",
       "        3.5807e-06, 1.4277e-08, 1.8995e-07, 5.9091e-06, 5.4166e-08, 4.4320e-04,\n",
       "        3.4978e-09, 3.3621e-07, 2.6284e-06, 1.2411e-07, 6.6852e-07, 3.3523e-07,\n",
       "        2.7918e-07, 6.7871e-09, 7.1093e-08, 3.1407e-07, 2.5655e-09, 1.5035e-08,\n",
       "        3.2635e-04, 1.9092e-07, 1.4963e-06, 2.6509e-07, 3.5077e-07, 3.7455e-06,\n",
       "        1.4633e-06, 7.6378e-07, 2.6133e-07, 5.6464e-08, 2.0177e-06, 3.1307e-07,\n",
       "        1.3737e-07, 3.9311e-08, 1.9320e-06, 4.3533e-07, 7.1258e-07, 6.1270e-08,\n",
       "        2.1381e-08, 9.6869e-08, 5.2768e-07, 3.6551e-07, 1.1818e-06, 2.2504e-06,\n",
       "        2.0120e-08, 2.5990e-07, 1.6341e-05, 4.1177e-07, 1.1542e-06, 8.3846e-07,\n",
       "        3.4323e-08, 7.8678e-09, 3.4087e-07, 2.6350e-06, 4.3905e-07, 1.2053e-06,\n",
       "        2.9167e-07, 1.3801e-07, 4.2539e-08, 1.5446e-06, 2.2243e-08, 8.0258e-08,\n",
       "        5.0777e-07, 1.6364e-08, 3.8129e-06, 2.8083e-07, 2.1631e-07, 6.6866e-08,\n",
       "        1.8718e-07, 7.2352e-06, 2.3720e-08, 5.7181e-09, 3.6450e-09, 1.7904e-08,\n",
       "        1.5354e-07, 4.6299e-08, 5.0674e-06, 2.0743e-07, 8.1308e-07, 6.0656e-07,\n",
       "        8.0143e-10, 5.9790e-07, 1.5740e-07, 1.9828e-07, 6.0375e-07, 3.8494e-07,\n",
       "        3.5783e-08, 1.3306e-07, 5.0487e-07, 2.8809e-06, 9.8008e-07, 7.7840e-07,\n",
       "        9.5707e-09, 1.4929e-07, 3.2443e-07, 1.7590e-06, 2.4751e-06, 1.3119e-06,\n",
       "        1.4616e-06, 2.4786e-06, 2.2205e-07, 2.3041e-07, 2.4017e-08, 3.8309e-08,\n",
       "        2.3200e-06, 3.4847e-06, 5.0746e-07, 4.8162e-07, 3.8389e-06, 4.7665e-07,\n",
       "        5.7812e-08, 1.1551e-07, 8.2243e-07, 4.1129e-07, 9.6038e-06, 1.2877e-07,\n",
       "        1.8546e-08, 8.4161e-07, 3.7785e-07, 1.4869e-07, 1.8006e-06, 1.2212e-05,\n",
       "        2.7996e-08, 9.3314e-06, 6.8460e-07, 2.1549e-04, 1.7287e-06, 2.4089e-07,\n",
       "        7.8090e-07, 3.2970e-08, 1.7867e-08, 4.0151e-08, 3.8384e-07, 3.3740e-08,\n",
       "        3.4981e-08, 3.9424e-07, 5.2956e-08, 2.3109e-06, 7.9803e-08, 6.5671e-08,\n",
       "        3.4153e-07, 9.6709e-08, 9.3638e-06, 7.9931e-08, 8.7977e-07, 7.7233e-07,\n",
       "        2.5366e-08, 2.1768e-07, 6.7168e-07, 2.5002e-07, 1.2599e-06, 8.3545e-07,\n",
       "        9.4808e-09, 1.5285e-07, 1.5037e-09, 9.1293e-09, 2.6890e-07, 1.8244e-07,\n",
       "        1.1968e-08, 1.2976e-08, 1.4537e-06, 3.5785e-07, 2.0739e-07, 1.7640e-06,\n",
       "        3.6566e-06, 7.0403e-07, 7.7231e-07, 6.7923e-07, 1.7526e-04, 2.1644e-06,\n",
       "        2.3327e-08, 2.8002e-06, 4.7575e-07, 3.6589e-08, 1.9355e-07, 2.4190e-07,\n",
       "        3.1939e-07, 9.5756e-07, 1.6036e-08, 1.4493e-07, 7.3691e-08, 1.7363e-08,\n",
       "        1.1252e-07, 1.1189e-08, 2.3804e-05, 6.1623e-07, 3.9330e-06, 3.1567e-07,\n",
       "        3.5797e-08, 8.5299e-06, 2.6413e-06, 9.6094e-06, 4.4595e-07, 1.9895e-06,\n",
       "        3.2002e-09, 6.2708e-07, 1.0955e-05, 6.3009e-08, 3.9066e-07, 1.6917e-05,\n",
       "        1.1389e-08, 2.2445e-05, 6.6462e-09, 3.9533e-06, 2.5350e-08, 1.3252e-06,\n",
       "        1.9566e-07, 8.0772e-10, 2.2815e-07, 9.6091e-07, 6.7970e-07, 7.6190e-09,\n",
       "        1.0057e-04, 1.9497e-08, 6.4809e-08, 5.1517e-06, 8.2501e-07, 1.6665e-05,\n",
       "        2.8992e-06, 2.0876e-06, 8.3025e-07, 7.3262e-07, 1.5953e-06, 1.0995e-07,\n",
       "        3.8121e-06, 7.5481e-07, 9.5083e-07, 7.2051e-08, 4.3526e-06, 6.8174e-06,\n",
       "        2.2957e-08, 1.1557e-05, 5.2345e-06, 1.5898e-06, 1.8939e-07, 2.2107e-08,\n",
       "        1.3315e-06, 1.3319e-06, 5.1969e-07, 1.3108e-07, 3.5478e-07, 6.0741e-08,\n",
       "        5.9940e-07, 2.1261e-06, 1.6084e-07, 7.0540e-07, 5.0283e-08, 3.5927e-08,\n",
       "        1.2562e-06, 9.5381e-08, 7.1772e-07, 2.5626e-07, 4.2187e-07, 1.5550e-07,\n",
       "        2.7054e-08, 8.4569e-07, 2.3118e-07, 7.7246e-08, 9.6023e-08, 1.5308e-06,\n",
       "        4.0268e-05, 1.2958e-06, 2.0157e-06, 6.2043e-07, 2.3225e-08, 1.2215e-05,\n",
       "        1.8632e-08, 1.2135e-06, 2.5051e-07, 1.7701e-07, 3.8656e-07, 6.1824e-07,\n",
       "        1.2091e-07, 8.6274e-06, 1.9432e-06, 6.1561e-08, 7.0178e-07, 2.5917e-08,\n",
       "        1.0575e-08, 5.9257e-06, 5.4708e-08, 7.5989e-07, 5.6697e-07, 6.2964e-07,\n",
       "        1.1855e-06, 2.1447e-07, 3.9729e-07, 1.8575e-07, 5.0188e-08, 3.1429e-07,\n",
       "        5.0815e-08, 7.9875e-06, 7.2489e-06, 3.1814e-06, 1.2824e-07, 1.2036e-06,\n",
       "        2.0394e-06, 9.0587e-09, 8.0258e-07, 1.3272e-06, 2.2552e-06, 7.3384e-06,\n",
       "        1.5290e-07, 6.3173e-06, 7.7634e-07, 3.1748e-06, 1.6701e-06, 5.2981e-06,\n",
       "        1.5568e-06, 5.2603e-07, 5.6683e-08, 5.8482e-08, 5.7475e-07, 3.1360e-06,\n",
       "        2.4833e-07, 1.5567e-06, 1.4153e-07, 2.1851e-07, 5.3855e-07, 1.4571e-06,\n",
       "        6.2096e-05, 2.2086e-07, 1.3770e-07, 7.9232e-07, 1.7983e-08, 3.4416e-08,\n",
       "        3.5489e-08, 7.6532e-09, 1.2794e-08, 4.9329e-07, 2.4016e-07, 2.3584e-07,\n",
       "        1.0688e-07, 3.9267e-09, 2.2619e-08, 5.6265e-07, 3.9889e-07, 1.3139e-06,\n",
       "        2.6150e-06, 1.1696e-06, 2.1093e-07, 6.3823e-08, 9.9234e-10, 5.7741e-07,\n",
       "        2.8332e-05, 1.2564e-07, 1.9252e-08, 3.7256e-05, 1.6249e-06, 1.6345e-07,\n",
       "        4.5495e-06, 3.1087e-07, 7.3882e-08, 1.1823e-07, 8.0711e-08, 6.0379e-09,\n",
       "        2.9084e-07, 1.2606e-06, 2.7139e-08, 1.0366e-08, 4.0560e-07, 1.8150e-07,\n",
       "        7.3705e-08, 1.0584e-06, 5.9977e-07, 3.3208e-06, 4.8708e-05, 1.3009e-06,\n",
       "        8.1449e-08, 4.1840e-08, 7.6198e-08, 2.8049e-06, 5.0465e-06, 1.5032e-07,\n",
       "        1.4891e-07, 2.2690e-06, 4.8566e-06, 5.1222e-07, 2.1898e-06, 2.4640e-06,\n",
       "        1.2036e-05, 9.8808e-07, 1.3318e-07, 7.2314e-08, 1.0212e-07, 1.2166e-09,\n",
       "        2.8291e-07, 1.3156e-07, 1.0863e-07, 7.3495e-08, 3.1576e-08, 9.9845e-08,\n",
       "        5.6839e-08, 4.3567e-09, 2.6245e-10, 6.0702e-08, 4.7208e-08, 3.8279e-05,\n",
       "        4.0712e-07, 8.1584e-08, 4.9661e-06, 1.9198e-08, 7.2932e-07, 1.7520e-06,\n",
       "        5.8303e-07, 1.2819e-06, 9.5600e-08, 9.4782e-07, 1.7572e-06, 1.1092e-08,\n",
       "        5.8419e-07, 3.5700e-07, 4.9284e-08, 1.4436e-08, 2.4408e-08, 8.3980e-08,\n",
       "        1.0465e-05, 4.2027e-06, 2.9578e-07, 4.5666e-07, 3.1691e-08, 1.5490e-06,\n",
       "        2.0773e-07, 1.7518e-07, 1.1898e-08, 6.5625e-07, 2.7570e-05, 8.4201e-08,\n",
       "        5.8728e-08, 2.5219e-06, 9.1920e-08, 5.5542e-08, 8.7648e-10, 4.0498e-08,\n",
       "        5.1094e-07, 6.5865e-10, 4.7914e-07, 6.7881e-07, 6.7748e-07, 5.6761e-06,\n",
       "        1.8833e-06, 3.5185e-08, 1.3558e-06, 1.9060e-05, 5.8537e-07, 5.8421e-06,\n",
       "        5.1946e-07, 8.1818e-08, 1.4543e-06, 3.8968e-08, 6.8626e-06, 4.5605e-08,\n",
       "        1.5777e-06, 3.0851e-07, 2.2754e-07, 2.9226e-06, 1.8070e-06, 7.3992e-08,\n",
       "        3.1219e-07, 2.3250e-07, 4.6932e-07, 2.9827e-10, 2.3096e-09, 6.0672e-09,\n",
       "        4.9260e-08, 7.3003e-09, 2.3617e-06, 4.2240e-05])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#file = open(\"raw_tensor_data\", \"wb\")\n",
    "#file.write(input_batch.data_ptr().to_bytes(150528, byteorder='little'))\n",
    "#file.close()\n",
    "\n",
    "with torch.no_grad():\n",
    "    output = model(input_tensor)\n",
    "\n",
    "torch.nn.functional.softmax(output[0], dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(258)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "values, indices = torch.max(output[0], 0)\n",
    "indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "chan_last_model = model.to(memory_format=torch.channels_last)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([3.4588e-07, 1.6758e-08, 2.7807e-08, 1.8452e-08, 4.7866e-08, 5.0434e-07,\n",
       "        3.3854e-07, 6.0622e-05, 1.1578e-03, 2.9473e-07, 2.6800e-10, 3.6728e-08,\n",
       "        4.8528e-10, 1.1868e-08, 2.4164e-09, 7.1094e-09, 6.4921e-08, 1.7225e-06,\n",
       "        5.8309e-07, 1.0582e-08, 1.8381e-08, 6.1718e-09, 3.2813e-08, 1.0519e-07,\n",
       "        2.2854e-08, 3.8548e-08, 2.5787e-08, 2.9939e-07, 4.3841e-08, 3.8215e-07,\n",
       "        3.9192e-07, 2.0235e-07, 1.0700e-07, 2.9171e-09, 4.6196e-08, 5.4040e-09,\n",
       "        5.4424e-08, 1.5754e-08, 6.8187e-08, 2.0042e-06, 3.1404e-08, 9.0537e-09,\n",
       "        7.4664e-08, 1.3874e-08, 3.6577e-07, 2.1438e-07, 9.0882e-07, 9.4028e-09,\n",
       "        1.3414e-07, 3.6072e-07, 2.6114e-06, 8.7284e-08, 8.9276e-08, 6.8340e-08,\n",
       "        1.7829e-07, 1.5330e-08, 1.2911e-07, 1.2032e-08, 1.0370e-07, 2.1345e-08,\n",
       "        1.2658e-06, 4.7681e-09, 4.8863e-08, 5.0200e-10, 3.3416e-09, 1.1046e-08,\n",
       "        2.2036e-07, 3.8540e-08, 7.1286e-08, 2.0001e-08, 9.7509e-09, 1.7718e-07,\n",
       "        8.6142e-09, 7.5555e-09, 2.3858e-08, 2.3912e-08, 3.7059e-08, 3.2888e-08,\n",
       "        7.7016e-07, 1.2912e-06, 2.0248e-08, 5.0239e-07, 1.0500e-06, 1.1112e-06,\n",
       "        1.9576e-05, 4.7071e-07, 3.1827e-07, 7.4484e-08, 2.6671e-08, 5.1705e-05,\n",
       "        2.6019e-08, 5.7953e-09, 4.5540e-09, 1.5768e-08, 1.1100e-08, 2.7368e-10,\n",
       "        2.0644e-07, 1.2930e-08, 4.8475e-10, 1.2373e-06, 6.3006e-07, 1.5019e-09,\n",
       "        2.3638e-07, 7.3496e-09, 9.3916e-03, 4.2287e-07, 1.0867e-05, 2.4083e-07,\n",
       "        5.9518e-07, 1.1934e-08, 1.6594e-07, 4.3151e-09, 1.8677e-07, 1.0006e-06,\n",
       "        3.4768e-06, 6.5114e-07, 8.7181e-08, 3.1819e-08, 5.5613e-07, 4.0118e-07,\n",
       "        8.7451e-10, 3.2588e-07, 1.6302e-07, 9.0966e-07, 3.0464e-06, 1.8273e-08,\n",
       "        1.5560e-07, 2.0542e-07, 3.6142e-09, 7.0172e-07, 8.0378e-09, 2.4653e-08,\n",
       "        1.1285e-06, 1.4637e-09, 3.7899e-07, 1.4335e-08, 1.8332e-08, 4.7599e-09,\n",
       "        5.4112e-08, 2.7008e-10, 9.3663e-10, 8.5073e-09, 4.0884e-09, 9.1284e-09,\n",
       "        2.1605e-07, 1.3305e-06, 6.6148e-07, 5.8239e-09, 1.7857e-08, 6.1936e-08,\n",
       "        1.5177e-08, 5.2299e-04, 3.0764e-05, 1.7434e-04, 9.6025e-05, 6.5942e-06,\n",
       "        3.4924e-07, 3.5151e-04, 5.2992e-06, 3.7823e-07, 4.4061e-07, 1.5427e-07,\n",
       "        3.1670e-07, 4.9234e-08, 3.0582e-08, 6.1569e-09, 9.7921e-08, 8.8216e-09,\n",
       "        4.3278e-07, 1.5218e-05, 7.7547e-05, 3.0628e-06, 7.5358e-08, 1.0160e-04,\n",
       "        3.4136e-04, 1.0291e-06, 1.4072e-08, 5.1319e-06, 5.2992e-07, 6.1060e-07,\n",
       "        4.1389e-06, 5.6265e-07, 6.1476e-06, 1.0719e-06, 2.3368e-05, 1.4419e-04,\n",
       "        5.8529e-04, 9.4565e-07, 4.2788e-05, 3.6274e-06, 2.8182e-05, 4.0939e-07,\n",
       "        7.7479e-04, 2.7515e-04, 1.6749e-06, 5.4207e-06, 2.5932e-06, 2.7640e-06,\n",
       "        2.3125e-06, 2.3777e-03, 2.9237e-06, 4.9139e-06, 2.0118e-06, 3.8964e-02,\n",
       "        7.5962e-06, 1.1020e-06, 2.3458e-07, 2.6584e-05, 1.4199e-05, 9.9378e-07,\n",
       "        3.9849e-08, 3.2151e-06, 7.4593e-06, 5.7125e-07, 8.2522e-08, 5.1237e-06,\n",
       "        1.1917e-05, 3.9491e-06, 1.3756e-06, 1.5220e-05, 9.1876e-07, 3.3611e-07,\n",
       "        1.9377e-04, 7.5630e-03, 7.0728e-04, 9.7548e-06, 1.6086e-05, 1.3675e-04,\n",
       "        4.0072e-05, 1.9999e-04, 5.9511e-03, 4.1076e-03, 6.6592e-04, 2.5135e-06,\n",
       "        6.4200e-07, 3.9969e-05, 3.5803e-06, 1.2806e-05, 1.3688e-06, 2.9189e-06,\n",
       "        1.6110e-06, 8.5992e-07, 3.8063e-07, 4.3043e-08, 1.2086e-07, 6.1855e-07,\n",
       "        5.1866e-07, 2.5972e-06, 3.0533e-03, 1.3632e-03, 2.5050e-03, 3.4382e-06,\n",
       "        6.0442e-07, 8.4570e-06, 2.8983e-07, 2.2762e-07, 3.8448e-05, 5.2614e-03,\n",
       "        6.1352e-01, 2.4764e-01, 2.8910e-04, 2.8367e-02, 7.0085e-08, 7.1633e-04,\n",
       "        2.2227e-05, 2.9674e-05, 5.2385e-05, 4.5794e-05, 1.0057e-06, 3.0264e-05,\n",
       "        4.1437e-03, 7.7938e-07, 9.0683e-06, 1.2423e-05, 2.3463e-06, 1.9848e-08,\n",
       "        4.8619e-07, 7.4776e-05, 4.4040e-05, 9.5176e-03, 3.7890e-05, 1.8331e-05,\n",
       "        8.7283e-06, 9.2028e-04, 3.3087e-04, 2.8071e-05, 8.5568e-08, 5.6468e-05,\n",
       "        2.0912e-07, 1.2506e-06, 1.0068e-08, 4.1083e-08, 2.1815e-07, 9.9944e-08,\n",
       "        4.7995e-08, 7.8822e-08, 4.6934e-07, 5.9374e-09, 5.3364e-07, 1.7249e-07,\n",
       "        4.1689e-10, 2.8510e-08, 2.2348e-08, 7.3156e-08, 3.6403e-08, 1.8468e-08,\n",
       "        2.0620e-07, 2.1629e-07, 6.8920e-09, 5.1294e-08, 1.1253e-06, 2.4103e-07,\n",
       "        2.1233e-07, 1.2732e-06, 5.7855e-07, 1.0633e-08, 4.4505e-08, 1.0890e-07,\n",
       "        3.3261e-07, 1.0333e-09, 1.8212e-09, 1.0661e-08, 8.5956e-09, 8.9955e-09,\n",
       "        3.3173e-08, 2.1016e-07, 5.9054e-08, 2.3140e-07, 1.0081e-07, 1.4771e-06,\n",
       "        4.0755e-05, 7.8032e-04, 2.1629e-03, 7.2501e-05, 7.0938e-06, 1.1337e-05,\n",
       "        1.2605e-05, 4.7027e-07, 2.9803e-05, 8.4342e-08, 9.8009e-09, 6.3739e-06,\n",
       "        5.3190e-07, 1.5374e-08, 1.9261e-08, 1.6387e-07, 3.9317e-08, 6.0420e-08,\n",
       "        6.6254e-05, 2.5534e-07, 2.0401e-07, 1.1508e-09, 3.7711e-09, 2.7960e-07,\n",
       "        8.5450e-09, 2.8278e-04, 8.1171e-05, 1.9749e-06, 3.7429e-05, 1.3434e-04,\n",
       "        3.7030e-07, 4.5713e-06, 9.8947e-07, 1.9453e-06, 4.9782e-08, 3.0886e-08,\n",
       "        2.0294e-08, 4.6394e-09, 1.4828e-07, 6.5780e-09, 9.1422e-08, 3.9910e-07,\n",
       "        3.0020e-07, 4.8781e-06, 3.2279e-07, 2.0162e-08, 1.3642e-09, 2.5538e-06,\n",
       "        3.1528e-07, 2.4363e-08, 6.6099e-07, 2.4612e-07, 2.4258e-07, 3.1354e-07,\n",
       "        2.1043e-07, 6.9363e-10, 2.1253e-10, 7.6771e-08, 7.4070e-08, 6.5995e-08,\n",
       "        8.1410e-08, 8.8999e-07, 2.1720e-09, 2.4160e-07, 1.1413e-08, 1.0864e-06,\n",
       "        5.0749e-08, 1.7534e-07, 9.4957e-08, 3.9117e-08, 2.8506e-09, 1.3096e-07,\n",
       "        3.5561e-06, 8.9556e-08, 4.1686e-08, 3.0531e-07, 2.0999e-08, 1.9460e-07,\n",
       "        1.1210e-06, 1.7864e-07, 3.7629e-09, 3.8802e-06, 6.6175e-06, 5.8098e-07,\n",
       "        5.1556e-08, 3.9367e-07, 1.6453e-08, 2.7844e-07, 6.0171e-08, 4.1792e-07,\n",
       "        1.7345e-06, 6.3264e-07, 6.6522e-08, 1.9265e-08, 4.2478e-08, 2.5694e-06,\n",
       "        5.2307e-07, 5.5518e-06, 3.6302e-05, 1.4108e-06, 5.5076e-09, 2.9517e-07,\n",
       "        5.4051e-07, 5.1031e-08, 1.1496e-05, 2.3590e-06, 1.7910e-06, 4.4031e-06,\n",
       "        1.3740e-08, 7.9657e-08, 2.3676e-05, 8.4320e-07, 6.0824e-07, 6.6298e-07,\n",
       "        1.3728e-06, 5.0817e-07, 6.3191e-07, 2.0869e-06, 1.0816e-07, 2.1259e-07,\n",
       "        1.0318e-06, 1.6202e-08, 7.1163e-07, 1.6559e-07, 1.0681e-08, 3.1327e-07,\n",
       "        4.9423e-07, 3.9095e-06, 2.3738e-06, 2.1045e-07, 3.4557e-05, 1.2212e-06,\n",
       "        1.2253e-05, 1.3568e-05, 1.7816e-07, 1.2495e-08, 4.5366e-09, 8.6975e-09,\n",
       "        3.7425e-07, 6.2431e-07, 1.8978e-06, 3.7473e-06, 3.2393e-06, 7.1507e-08,\n",
       "        9.1099e-08, 2.7222e-08, 7.3638e-08, 1.9595e-07, 7.1825e-06, 4.7649e-07,\n",
       "        1.5025e-07, 2.8873e-08, 8.0239e-08, 1.0239e-05, 5.2914e-06, 2.7670e-07,\n",
       "        3.0429e-08, 2.3809e-07, 2.3746e-06, 5.7694e-05, 1.6945e-06, 9.2531e-07,\n",
       "        1.1828e-06, 9.3504e-07, 5.8272e-07, 5.7519e-07, 2.6195e-05, 1.6684e-06,\n",
       "        1.9488e-06, 3.2853e-07, 9.9031e-09, 3.3212e-06, 1.2782e-05, 1.0810e-06,\n",
       "        2.0878e-07, 9.4568e-07, 1.1269e-06, 3.6654e-07, 4.8289e-07, 2.4275e-08,\n",
       "        2.5038e-08, 9.6285e-07, 6.1180e-07, 2.2492e-06, 1.5271e-07, 2.7282e-06,\n",
       "        6.9635e-07, 3.1523e-08, 5.1758e-08, 3.1880e-06, 1.1723e-07, 1.2840e-07,\n",
       "        2.8011e-05, 2.5181e-06, 3.1831e-07, 3.7933e-07, 7.9372e-09, 3.5694e-08,\n",
       "        3.9637e-08, 5.8592e-07, 5.7830e-07, 1.8769e-08, 3.8251e-08, 6.3533e-07,\n",
       "        3.5807e-06, 1.4277e-08, 1.8995e-07, 5.9091e-06, 5.4166e-08, 4.4320e-04,\n",
       "        3.4978e-09, 3.3621e-07, 2.6284e-06, 1.2411e-07, 6.6852e-07, 3.3523e-07,\n",
       "        2.7918e-07, 6.7871e-09, 7.1093e-08, 3.1407e-07, 2.5655e-09, 1.5035e-08,\n",
       "        3.2635e-04, 1.9092e-07, 1.4963e-06, 2.6509e-07, 3.5077e-07, 3.7455e-06,\n",
       "        1.4633e-06, 7.6378e-07, 2.6133e-07, 5.6464e-08, 2.0177e-06, 3.1307e-07,\n",
       "        1.3737e-07, 3.9311e-08, 1.9320e-06, 4.3533e-07, 7.1258e-07, 6.1270e-08,\n",
       "        2.1381e-08, 9.6869e-08, 5.2768e-07, 3.6551e-07, 1.1818e-06, 2.2504e-06,\n",
       "        2.0120e-08, 2.5990e-07, 1.6341e-05, 4.1177e-07, 1.1542e-06, 8.3846e-07,\n",
       "        3.4323e-08, 7.8678e-09, 3.4087e-07, 2.6350e-06, 4.3905e-07, 1.2053e-06,\n",
       "        2.9167e-07, 1.3801e-07, 4.2539e-08, 1.5446e-06, 2.2243e-08, 8.0258e-08,\n",
       "        5.0777e-07, 1.6364e-08, 3.8129e-06, 2.8083e-07, 2.1631e-07, 6.6866e-08,\n",
       "        1.8718e-07, 7.2352e-06, 2.3720e-08, 5.7181e-09, 3.6450e-09, 1.7904e-08,\n",
       "        1.5354e-07, 4.6299e-08, 5.0674e-06, 2.0743e-07, 8.1308e-07, 6.0656e-07,\n",
       "        8.0143e-10, 5.9790e-07, 1.5740e-07, 1.9828e-07, 6.0375e-07, 3.8494e-07,\n",
       "        3.5783e-08, 1.3306e-07, 5.0487e-07, 2.8809e-06, 9.8008e-07, 7.7840e-07,\n",
       "        9.5707e-09, 1.4929e-07, 3.2443e-07, 1.7590e-06, 2.4751e-06, 1.3119e-06,\n",
       "        1.4616e-06, 2.4786e-06, 2.2205e-07, 2.3041e-07, 2.4017e-08, 3.8309e-08,\n",
       "        2.3200e-06, 3.4847e-06, 5.0746e-07, 4.8162e-07, 3.8389e-06, 4.7665e-07,\n",
       "        5.7812e-08, 1.1551e-07, 8.2243e-07, 4.1129e-07, 9.6038e-06, 1.2877e-07,\n",
       "        1.8546e-08, 8.4161e-07, 3.7785e-07, 1.4869e-07, 1.8006e-06, 1.2212e-05,\n",
       "        2.7996e-08, 9.3314e-06, 6.8460e-07, 2.1549e-04, 1.7287e-06, 2.4089e-07,\n",
       "        7.8090e-07, 3.2970e-08, 1.7867e-08, 4.0151e-08, 3.8384e-07, 3.3740e-08,\n",
       "        3.4981e-08, 3.9424e-07, 5.2956e-08, 2.3109e-06, 7.9803e-08, 6.5671e-08,\n",
       "        3.4153e-07, 9.6709e-08, 9.3638e-06, 7.9931e-08, 8.7977e-07, 7.7233e-07,\n",
       "        2.5366e-08, 2.1768e-07, 6.7168e-07, 2.5002e-07, 1.2599e-06, 8.3545e-07,\n",
       "        9.4808e-09, 1.5285e-07, 1.5037e-09, 9.1293e-09, 2.6890e-07, 1.8244e-07,\n",
       "        1.1968e-08, 1.2976e-08, 1.4537e-06, 3.5785e-07, 2.0739e-07, 1.7640e-06,\n",
       "        3.6566e-06, 7.0403e-07, 7.7231e-07, 6.7923e-07, 1.7526e-04, 2.1644e-06,\n",
       "        2.3327e-08, 2.8002e-06, 4.7575e-07, 3.6589e-08, 1.9355e-07, 2.4190e-07,\n",
       "        3.1939e-07, 9.5756e-07, 1.6036e-08, 1.4493e-07, 7.3691e-08, 1.7363e-08,\n",
       "        1.1252e-07, 1.1189e-08, 2.3804e-05, 6.1623e-07, 3.9330e-06, 3.1567e-07,\n",
       "        3.5797e-08, 8.5299e-06, 2.6413e-06, 9.6094e-06, 4.4595e-07, 1.9895e-06,\n",
       "        3.2002e-09, 6.2708e-07, 1.0955e-05, 6.3009e-08, 3.9066e-07, 1.6917e-05,\n",
       "        1.1389e-08, 2.2445e-05, 6.6462e-09, 3.9533e-06, 2.5350e-08, 1.3252e-06,\n",
       "        1.9566e-07, 8.0772e-10, 2.2815e-07, 9.6091e-07, 6.7970e-07, 7.6190e-09,\n",
       "        1.0057e-04, 1.9497e-08, 6.4809e-08, 5.1517e-06, 8.2501e-07, 1.6665e-05,\n",
       "        2.8992e-06, 2.0876e-06, 8.3025e-07, 7.3262e-07, 1.5953e-06, 1.0995e-07,\n",
       "        3.8121e-06, 7.5481e-07, 9.5083e-07, 7.2051e-08, 4.3526e-06, 6.8174e-06,\n",
       "        2.2957e-08, 1.1557e-05, 5.2345e-06, 1.5898e-06, 1.8939e-07, 2.2107e-08,\n",
       "        1.3315e-06, 1.3319e-06, 5.1969e-07, 1.3108e-07, 3.5478e-07, 6.0741e-08,\n",
       "        5.9940e-07, 2.1261e-06, 1.6084e-07, 7.0540e-07, 5.0283e-08, 3.5927e-08,\n",
       "        1.2562e-06, 9.5381e-08, 7.1772e-07, 2.5626e-07, 4.2187e-07, 1.5550e-07,\n",
       "        2.7054e-08, 8.4569e-07, 2.3118e-07, 7.7246e-08, 9.6023e-08, 1.5308e-06,\n",
       "        4.0268e-05, 1.2958e-06, 2.0157e-06, 6.2043e-07, 2.3225e-08, 1.2215e-05,\n",
       "        1.8632e-08, 1.2135e-06, 2.5051e-07, 1.7701e-07, 3.8656e-07, 6.1824e-07,\n",
       "        1.2091e-07, 8.6274e-06, 1.9432e-06, 6.1561e-08, 7.0178e-07, 2.5917e-08,\n",
       "        1.0575e-08, 5.9257e-06, 5.4708e-08, 7.5989e-07, 5.6697e-07, 6.2964e-07,\n",
       "        1.1855e-06, 2.1447e-07, 3.9729e-07, 1.8575e-07, 5.0188e-08, 3.1429e-07,\n",
       "        5.0815e-08, 7.9875e-06, 7.2489e-06, 3.1814e-06, 1.2824e-07, 1.2036e-06,\n",
       "        2.0394e-06, 9.0587e-09, 8.0258e-07, 1.3272e-06, 2.2552e-06, 7.3384e-06,\n",
       "        1.5290e-07, 6.3173e-06, 7.7634e-07, 3.1748e-06, 1.6701e-06, 5.2981e-06,\n",
       "        1.5568e-06, 5.2603e-07, 5.6683e-08, 5.8482e-08, 5.7475e-07, 3.1360e-06,\n",
       "        2.4833e-07, 1.5567e-06, 1.4153e-07, 2.1851e-07, 5.3855e-07, 1.4571e-06,\n",
       "        6.2096e-05, 2.2086e-07, 1.3770e-07, 7.9232e-07, 1.7983e-08, 3.4416e-08,\n",
       "        3.5489e-08, 7.6532e-09, 1.2794e-08, 4.9329e-07, 2.4016e-07, 2.3584e-07,\n",
       "        1.0688e-07, 3.9267e-09, 2.2619e-08, 5.6265e-07, 3.9889e-07, 1.3139e-06,\n",
       "        2.6150e-06, 1.1696e-06, 2.1093e-07, 6.3823e-08, 9.9234e-10, 5.7741e-07,\n",
       "        2.8332e-05, 1.2564e-07, 1.9252e-08, 3.7256e-05, 1.6249e-06, 1.6345e-07,\n",
       "        4.5495e-06, 3.1087e-07, 7.3882e-08, 1.1823e-07, 8.0711e-08, 6.0379e-09,\n",
       "        2.9084e-07, 1.2606e-06, 2.7139e-08, 1.0366e-08, 4.0560e-07, 1.8150e-07,\n",
       "        7.3705e-08, 1.0584e-06, 5.9977e-07, 3.3208e-06, 4.8708e-05, 1.3009e-06,\n",
       "        8.1449e-08, 4.1840e-08, 7.6198e-08, 2.8049e-06, 5.0465e-06, 1.5032e-07,\n",
       "        1.4891e-07, 2.2690e-06, 4.8566e-06, 5.1222e-07, 2.1898e-06, 2.4640e-06,\n",
       "        1.2036e-05, 9.8808e-07, 1.3318e-07, 7.2314e-08, 1.0212e-07, 1.2166e-09,\n",
       "        2.8291e-07, 1.3156e-07, 1.0863e-07, 7.3495e-08, 3.1576e-08, 9.9845e-08,\n",
       "        5.6839e-08, 4.3567e-09, 2.6245e-10, 6.0702e-08, 4.7208e-08, 3.8279e-05,\n",
       "        4.0712e-07, 8.1584e-08, 4.9661e-06, 1.9198e-08, 7.2932e-07, 1.7520e-06,\n",
       "        5.8303e-07, 1.2819e-06, 9.5600e-08, 9.4782e-07, 1.7572e-06, 1.1092e-08,\n",
       "        5.8419e-07, 3.5700e-07, 4.9284e-08, 1.4436e-08, 2.4408e-08, 8.3980e-08,\n",
       "        1.0465e-05, 4.2027e-06, 2.9578e-07, 4.5666e-07, 3.1691e-08, 1.5490e-06,\n",
       "        2.0773e-07, 1.7518e-07, 1.1898e-08, 6.5625e-07, 2.7570e-05, 8.4201e-08,\n",
       "        5.8728e-08, 2.5219e-06, 9.1920e-08, 5.5542e-08, 8.7648e-10, 4.0498e-08,\n",
       "        5.1094e-07, 6.5865e-10, 4.7914e-07, 6.7881e-07, 6.7748e-07, 5.6761e-06,\n",
       "        1.8833e-06, 3.5185e-08, 1.3558e-06, 1.9060e-05, 5.8537e-07, 5.8421e-06,\n",
       "        5.1946e-07, 8.1818e-08, 1.4543e-06, 3.8968e-08, 6.8626e-06, 4.5605e-08,\n",
       "        1.5777e-06, 3.0851e-07, 2.2754e-07, 2.9226e-06, 1.8070e-06, 7.3992e-08,\n",
       "        3.1219e-07, 2.3250e-07, 4.6932e-07, 2.9827e-10, 2.3096e-09, 6.0672e-09,\n",
       "        4.9260e-08, 7.3003e-09, 2.3617e-06, 4.2240e-05])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_tensor = torch.from_numpy(final_pre_process)\n",
    "last_input_tensor = input_tensor.to(memory_format=torch.channels_last)\n",
    "input_batch = last_input_tensor.unsqueeze(0) # create a mini-batch as expected by the model\n",
    "with torch.no_grad():\n",
    "    output = chan_last_model(input_tensor)\n",
    "\n",
    "torch.nn.functional.softmax(output[0], dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(258)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "values, indices = torch.max(output[0], 0)\n",
    "values\n",
    "indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#NCHW Model\n",
    "scriptedm = torch.jit.script(model)\n",
    "nchw_script_module_optimized = optimize_for_mobile(scriptedm)\n",
    "torch.jit.save(nchw_script_module_optimized, \"mobilenet_v2.pt\")\n",
    "\n",
    "#NHWC Model\n",
    "traced_script_module = torch.jit.script(chan_last_model)\n",
    "traced_script_module_optimized = optimize_for_mobile(traced_script_module)\n",
    "torch.jit.save(traced_script_module_optimized, \"mobilenet_v2_nhwc.pt\")\n",
    "\n",
    "#Vulkan NCHW Backend\n",
    "nchw_script_module_optimized = optimize_for_mobile(scriptedm, backend='vulkan')\n",
    "torch.jit.save(nchw_script_module_optimized, \"mobilenet_v2_vulkan_nchw.pt\")\n",
    "\n",
    "#Vulkan NHWC Backend\n",
    "traced_script_module_vulkan_optimized = optimize_for_mobile(traced_script_module, backend='vulkan')\n",
    "torch.jit.save(traced_script_module_vulkan_optimized, \"mobilenet_v2_vulkan_nhwc.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn(1, 3, 224, 224, requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.onnx.export(model,               # model being run\n",
    "                  x,                         # model input (or a tuple for multiple inputs)\n",
    "                  \"mobilenet_v2.onnx\",   # where to save the model (can be a file or file-like object)\n",
    "                  export_params=True,        # store the trained parameter weights inside the model file\n",
    "                  opset_version=10,          # the ONNX version to export the model to\n",
    "                  do_constant_folding=True,  # whether to execute constant folding for optimization\n",
    "                  input_names = ['input'],   # the model's input names\n",
    "                  output_names = ['output'], # the model's output names\n",
    "                  dynamic_axes={'input' : {0 : 'batch_size'},    # variable lenght axes\n",
    "                                'output' : {0 : 'batch_size'}})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_channels_last_model = torch.jit.load(\"mobilenet_v2_nhwc.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RecursiveScriptModule(original_name=MobileNetV2)\n"
     ]
    }
   ],
   "source": [
    "print(test_channels_last_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RecursiveScriptModule(original_name=MobileNetV2)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_channels_last_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/site-packages/torch/nn/modules/module.py:889: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  ../c10/core/TensorImpl.h:934.)\n",
      "  result = self.forward(*input, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([3.4588e-07, 1.6758e-08, 2.7807e-08, 1.8452e-08, 4.7865e-08, 5.0434e-07,\n",
       "        3.3854e-07, 6.0621e-05, 1.1578e-03, 2.9473e-07, 2.6800e-10, 3.6727e-08,\n",
       "        4.8527e-10, 1.1868e-08, 2.4163e-09, 7.1093e-09, 6.4921e-08, 1.7225e-06,\n",
       "        5.8308e-07, 1.0582e-08, 1.8381e-08, 6.1717e-09, 3.2813e-08, 1.0519e-07,\n",
       "        2.2853e-08, 3.8548e-08, 2.5787e-08, 2.9939e-07, 4.3840e-08, 3.8215e-07,\n",
       "        3.9192e-07, 2.0235e-07, 1.0700e-07, 2.9171e-09, 4.6196e-08, 5.4039e-09,\n",
       "        5.4423e-08, 1.5754e-08, 6.8186e-08, 2.0042e-06, 3.1403e-08, 9.0536e-09,\n",
       "        7.4663e-08, 1.3874e-08, 3.6577e-07, 2.1438e-07, 9.0880e-07, 9.4027e-09,\n",
       "        1.3414e-07, 3.6072e-07, 2.6113e-06, 8.7284e-08, 8.9275e-08, 6.8339e-08,\n",
       "        1.7828e-07, 1.5329e-08, 1.2911e-07, 1.2032e-08, 1.0370e-07, 2.1345e-08,\n",
       "        1.2658e-06, 4.7681e-09, 4.8862e-08, 5.0200e-10, 3.3416e-09, 1.1046e-08,\n",
       "        2.2035e-07, 3.8540e-08, 7.1285e-08, 2.0001e-08, 9.7508e-09, 1.7718e-07,\n",
       "        8.6141e-09, 7.5554e-09, 2.3858e-08, 2.3912e-08, 3.7058e-08, 3.2888e-08,\n",
       "        7.7015e-07, 1.2912e-06, 2.0247e-08, 5.0238e-07, 1.0500e-06, 1.1112e-06,\n",
       "        1.9576e-05, 4.7070e-07, 3.1826e-07, 7.4483e-08, 2.6671e-08, 5.1704e-05,\n",
       "        2.6019e-08, 5.7952e-09, 4.5539e-09, 1.5768e-08, 1.1100e-08, 2.7367e-10,\n",
       "        2.0644e-07, 1.2930e-08, 4.8475e-10, 1.2373e-06, 6.3005e-07, 1.5019e-09,\n",
       "        2.3637e-07, 7.3495e-09, 9.3915e-03, 4.2286e-07, 1.0867e-05, 2.4083e-07,\n",
       "        5.9517e-07, 1.1934e-08, 1.6594e-07, 4.3150e-09, 1.8677e-07, 1.0005e-06,\n",
       "        3.4768e-06, 6.5113e-07, 8.7180e-08, 3.1818e-08, 5.5612e-07, 4.0118e-07,\n",
       "        8.7449e-10, 3.2587e-07, 1.6302e-07, 9.0964e-07, 3.0464e-06, 1.8273e-08,\n",
       "        1.5560e-07, 2.0541e-07, 3.6141e-09, 7.0172e-07, 8.0377e-09, 2.4653e-08,\n",
       "        1.1285e-06, 1.4637e-09, 3.7898e-07, 1.4335e-08, 1.8332e-08, 4.7598e-09,\n",
       "        5.4111e-08, 2.7007e-10, 9.3662e-10, 8.5072e-09, 4.0883e-09, 9.1282e-09,\n",
       "        2.1605e-07, 1.3305e-06, 6.6146e-07, 5.8238e-09, 1.7857e-08, 6.1935e-08,\n",
       "        1.5177e-08, 5.2299e-04, 3.0763e-05, 1.7433e-04, 9.6024e-05, 6.5941e-06,\n",
       "        3.4924e-07, 3.5150e-04, 5.2992e-06, 3.7822e-07, 4.4060e-07, 1.5427e-07,\n",
       "        3.1669e-07, 4.9233e-08, 3.0582e-08, 6.1568e-09, 9.7920e-08, 8.8215e-09,\n",
       "        4.3278e-07, 1.5217e-05, 7.7546e-05, 3.0628e-06, 7.5357e-08, 1.0160e-04,\n",
       "        3.4135e-04, 1.0291e-06, 1.4072e-08, 5.1319e-06, 5.2991e-07, 6.1059e-07,\n",
       "        4.1388e-06, 5.6264e-07, 6.1475e-06, 1.0718e-06, 2.3368e-05, 1.4419e-04,\n",
       "        5.8528e-04, 9.4563e-07, 4.2787e-05, 3.6274e-06, 2.8182e-05, 4.0938e-07,\n",
       "        7.7478e-04, 2.7515e-04, 1.6749e-06, 5.4206e-06, 2.5932e-06, 2.7640e-06,\n",
       "        2.3124e-06, 2.3777e-03, 2.9237e-06, 4.9138e-06, 2.0117e-06, 3.8964e-02,\n",
       "        7.5961e-06, 1.1020e-06, 2.3458e-07, 2.6584e-05, 1.4199e-05, 9.9377e-07,\n",
       "        3.9848e-08, 3.2151e-06, 7.4592e-06, 5.7125e-07, 8.2521e-08, 5.1237e-06,\n",
       "        1.1917e-05, 3.9491e-06, 1.3756e-06, 1.5219e-05, 9.1875e-07, 3.3611e-07,\n",
       "        1.9376e-04, 7.5629e-03, 7.0728e-04, 9.7547e-06, 1.6086e-05, 1.3675e-04,\n",
       "        4.0071e-05, 1.9998e-04, 5.9511e-03, 4.1077e-03, 6.6591e-04, 2.5135e-06,\n",
       "        6.4199e-07, 3.9969e-05, 3.5803e-06, 1.2806e-05, 1.3688e-06, 2.9189e-06,\n",
       "        1.6110e-06, 8.5991e-07, 3.8062e-07, 4.3043e-08, 1.2086e-07, 6.1854e-07,\n",
       "        5.1866e-07, 2.5971e-06, 3.0532e-03, 1.3632e-03, 2.5050e-03, 3.4382e-06,\n",
       "        6.0441e-07, 8.4570e-06, 2.8983e-07, 2.2762e-07, 3.8447e-05, 5.2614e-03,\n",
       "        6.1353e-01, 2.4763e-01, 2.8910e-04, 2.8366e-02, 7.0085e-08, 7.1633e-04,\n",
       "        2.2226e-05, 2.9673e-05, 5.2384e-05, 4.5794e-05, 1.0057e-06, 3.0264e-05,\n",
       "        4.1437e-03, 7.7938e-07, 9.0683e-06, 1.2423e-05, 2.3463e-06, 1.9848e-08,\n",
       "        4.8619e-07, 7.4775e-05, 4.4040e-05, 9.5175e-03, 3.7890e-05, 1.8331e-05,\n",
       "        8.7282e-06, 9.2027e-04, 3.3086e-04, 2.8070e-05, 8.5568e-08, 5.6468e-05,\n",
       "        2.0912e-07, 1.2506e-06, 1.0068e-08, 4.1083e-08, 2.1815e-07, 9.9943e-08,\n",
       "        4.7995e-08, 7.8822e-08, 4.6934e-07, 5.9373e-09, 5.3364e-07, 1.7248e-07,\n",
       "        4.1688e-10, 2.8510e-08, 2.2348e-08, 7.3155e-08, 3.6403e-08, 1.8468e-08,\n",
       "        2.0620e-07, 2.1629e-07, 6.8919e-09, 5.1293e-08, 1.1253e-06, 2.4103e-07,\n",
       "        2.1233e-07, 1.2732e-06, 5.7854e-07, 1.0633e-08, 4.4504e-08, 1.0890e-07,\n",
       "        3.3260e-07, 1.0332e-09, 1.8212e-09, 1.0661e-08, 8.5955e-09, 8.9954e-09,\n",
       "        3.3172e-08, 2.1015e-07, 5.9053e-08, 2.3139e-07, 1.0080e-07, 1.4771e-06,\n",
       "        4.0754e-05, 7.8032e-04, 2.1628e-03, 7.2500e-05, 7.0936e-06, 1.1337e-05,\n",
       "        1.2605e-05, 4.7026e-07, 2.9803e-05, 8.4342e-08, 9.8008e-09, 6.3739e-06,\n",
       "        5.3190e-07, 1.5374e-08, 1.9261e-08, 1.6387e-07, 3.9316e-08, 6.0419e-08,\n",
       "        6.6253e-05, 2.5534e-07, 2.0400e-07, 1.1508e-09, 3.7711e-09, 2.7960e-07,\n",
       "        8.5449e-09, 2.8278e-04, 8.1169e-05, 1.9749e-06, 3.7428e-05, 1.3434e-04,\n",
       "        3.7029e-07, 4.5713e-06, 9.8946e-07, 1.9453e-06, 4.9782e-08, 3.0886e-08,\n",
       "        2.0293e-08, 4.6394e-09, 1.4828e-07, 6.5780e-09, 9.1421e-08, 3.9910e-07,\n",
       "        3.0019e-07, 4.8780e-06, 3.2279e-07, 2.0162e-08, 1.3642e-09, 2.5538e-06,\n",
       "        3.1528e-07, 2.4363e-08, 6.6098e-07, 2.4611e-07, 2.4258e-07, 3.1354e-07,\n",
       "        2.1043e-07, 6.9362e-10, 2.1252e-10, 7.6771e-08, 7.4069e-08, 6.5994e-08,\n",
       "        8.1409e-08, 8.8998e-07, 2.1719e-09, 2.4159e-07, 1.1413e-08, 1.0864e-06,\n",
       "        5.0748e-08, 1.7534e-07, 9.4955e-08, 3.9117e-08, 2.8506e-09, 1.3095e-07,\n",
       "        3.5560e-06, 8.9554e-08, 4.1685e-08, 3.0530e-07, 2.0999e-08, 1.9459e-07,\n",
       "        1.1209e-06, 1.7863e-07, 3.7629e-09, 3.8802e-06, 6.6174e-06, 5.8097e-07,\n",
       "        5.1556e-08, 3.9367e-07, 1.6453e-08, 2.7843e-07, 6.0171e-08, 4.1792e-07,\n",
       "        1.7345e-06, 6.3263e-07, 6.6522e-08, 1.9265e-08, 4.2478e-08, 2.5693e-06,\n",
       "        5.2307e-07, 5.5517e-06, 3.6302e-05, 1.4108e-06, 5.5075e-09, 2.9516e-07,\n",
       "        5.4051e-07, 5.1031e-08, 1.1496e-05, 2.3590e-06, 1.7910e-06, 4.4031e-06,\n",
       "        1.3740e-08, 7.9656e-08, 2.3676e-05, 8.4318e-07, 6.0823e-07, 6.6297e-07,\n",
       "        1.3728e-06, 5.0817e-07, 6.3190e-07, 2.0869e-06, 1.0816e-07, 2.1259e-07,\n",
       "        1.0318e-06, 1.6202e-08, 7.1162e-07, 1.6559e-07, 1.0681e-08, 3.1327e-07,\n",
       "        4.9422e-07, 3.9095e-06, 2.3738e-06, 2.1045e-07, 3.4556e-05, 1.2211e-06,\n",
       "        1.2253e-05, 1.3567e-05, 1.7816e-07, 1.2495e-08, 4.5366e-09, 8.6974e-09,\n",
       "        3.7425e-07, 6.2430e-07, 1.8978e-06, 3.7472e-06, 3.2393e-06, 7.1506e-08,\n",
       "        9.1097e-08, 2.7221e-08, 7.3638e-08, 1.9594e-07, 7.1824e-06, 4.7649e-07,\n",
       "        1.5025e-07, 2.8872e-08, 8.0237e-08, 1.0239e-05, 5.2913e-06, 2.7670e-07,\n",
       "        3.0428e-08, 2.3809e-07, 2.3745e-06, 5.7693e-05, 1.6945e-06, 9.2530e-07,\n",
       "        1.1828e-06, 9.3503e-07, 5.8271e-07, 5.7519e-07, 2.6194e-05, 1.6684e-06,\n",
       "        1.9488e-06, 3.2853e-07, 9.9028e-09, 3.3211e-06, 1.2782e-05, 1.0810e-06,\n",
       "        2.0878e-07, 9.4567e-07, 1.1269e-06, 3.6654e-07, 4.8289e-07, 2.4275e-08,\n",
       "        2.5038e-08, 9.6284e-07, 6.1180e-07, 2.2492e-06, 1.5271e-07, 2.7282e-06,\n",
       "        6.9634e-07, 3.1523e-08, 5.1757e-08, 3.1880e-06, 1.1723e-07, 1.2840e-07,\n",
       "        2.8010e-05, 2.5180e-06, 3.1831e-07, 3.7933e-07, 7.9371e-09, 3.5693e-08,\n",
       "        3.9636e-08, 5.8591e-07, 5.7830e-07, 1.8769e-08, 3.8250e-08, 6.3532e-07,\n",
       "        3.5807e-06, 1.4276e-08, 1.8995e-07, 5.9090e-06, 5.4165e-08, 4.4319e-04,\n",
       "        3.4977e-09, 3.3620e-07, 2.6284e-06, 1.2411e-07, 6.6852e-07, 3.3523e-07,\n",
       "        2.7917e-07, 6.7870e-09, 7.1093e-08, 3.1406e-07, 2.5655e-09, 1.5035e-08,\n",
       "        3.2635e-04, 1.9092e-07, 1.4963e-06, 2.6508e-07, 3.5077e-07, 3.7455e-06,\n",
       "        1.4633e-06, 7.6377e-07, 2.6132e-07, 5.6463e-08, 2.0177e-06, 3.1306e-07,\n",
       "        1.3737e-07, 3.9310e-08, 1.9320e-06, 4.3532e-07, 7.1258e-07, 6.1269e-08,\n",
       "        2.1380e-08, 9.6867e-08, 5.2767e-07, 3.6550e-07, 1.1818e-06, 2.2504e-06,\n",
       "        2.0120e-08, 2.5990e-07, 1.6341e-05, 4.1176e-07, 1.1542e-06, 8.3845e-07,\n",
       "        3.4323e-08, 7.8678e-09, 3.4087e-07, 2.6350e-06, 4.3904e-07, 1.2053e-06,\n",
       "        2.9167e-07, 1.3801e-07, 4.2538e-08, 1.5446e-06, 2.2242e-08, 8.0257e-08,\n",
       "        5.0776e-07, 1.6364e-08, 3.8128e-06, 2.8083e-07, 2.1631e-07, 6.6865e-08,\n",
       "        1.8718e-07, 7.2350e-06, 2.3720e-08, 5.7180e-09, 3.6449e-09, 1.7903e-08,\n",
       "        1.5354e-07, 4.6298e-08, 5.0673e-06, 2.0743e-07, 8.1307e-07, 6.0655e-07,\n",
       "        8.0142e-10, 5.9789e-07, 1.5740e-07, 1.9827e-07, 6.0374e-07, 3.8494e-07,\n",
       "        3.5783e-08, 1.3306e-07, 5.0486e-07, 2.8809e-06, 9.8007e-07, 7.7840e-07,\n",
       "        9.5706e-09, 1.4929e-07, 3.2442e-07, 1.7590e-06, 2.4750e-06, 1.3119e-06,\n",
       "        1.4616e-06, 2.4786e-06, 2.2205e-07, 2.3041e-07, 2.4017e-08, 3.8309e-08,\n",
       "        2.3200e-06, 3.4846e-06, 5.0745e-07, 4.8162e-07, 3.8389e-06, 4.7665e-07,\n",
       "        5.7811e-08, 1.1551e-07, 8.2242e-07, 4.1128e-07, 9.6037e-06, 1.2877e-07,\n",
       "        1.8546e-08, 8.4160e-07, 3.7784e-07, 1.4869e-07, 1.8006e-06, 1.2212e-05,\n",
       "        2.7996e-08, 9.3312e-06, 6.8459e-07, 2.1549e-04, 1.7286e-06, 2.4089e-07,\n",
       "        7.8089e-07, 3.2969e-08, 1.7867e-08, 4.0150e-08, 3.8384e-07, 3.3740e-08,\n",
       "        3.4980e-08, 3.9424e-07, 5.2955e-08, 2.3108e-06, 7.9801e-08, 6.5670e-08,\n",
       "        3.4153e-07, 9.6708e-08, 9.3637e-06, 7.9930e-08, 8.7977e-07, 7.7232e-07,\n",
       "        2.5366e-08, 2.1768e-07, 6.7167e-07, 2.5002e-07, 1.2599e-06, 8.3544e-07,\n",
       "        9.4807e-09, 1.5285e-07, 1.5036e-09, 9.1291e-09, 2.6890e-07, 1.8244e-07,\n",
       "        1.1968e-08, 1.2976e-08, 1.4537e-06, 3.5785e-07, 2.0739e-07, 1.7640e-06,\n",
       "        3.6565e-06, 7.0402e-07, 7.7230e-07, 6.7923e-07, 1.7525e-04, 2.1644e-06,\n",
       "        2.3327e-08, 2.8002e-06, 4.7575e-07, 3.6588e-08, 1.9355e-07, 2.4190e-07,\n",
       "        3.1938e-07, 9.5755e-07, 1.6036e-08, 1.4493e-07, 7.3690e-08, 1.7363e-08,\n",
       "        1.1252e-07, 1.1189e-08, 2.3804e-05, 6.1622e-07, 3.9329e-06, 3.1567e-07,\n",
       "        3.5796e-08, 8.5297e-06, 2.6413e-06, 9.6093e-06, 4.4594e-07, 1.9895e-06,\n",
       "        3.2002e-09, 6.2707e-07, 1.0954e-05, 6.3008e-08, 3.9065e-07, 1.6917e-05,\n",
       "        1.1389e-08, 2.2444e-05, 6.6461e-09, 3.9532e-06, 2.5350e-08, 1.3251e-06,\n",
       "        1.9566e-07, 8.0770e-10, 2.2815e-07, 9.6089e-07, 6.7969e-07, 7.6189e-09,\n",
       "        1.0057e-04, 1.9497e-08, 6.4809e-08, 5.1516e-06, 8.2501e-07, 1.6664e-05,\n",
       "        2.8991e-06, 2.0876e-06, 8.3024e-07, 7.3261e-07, 1.5953e-06, 1.0995e-07,\n",
       "        3.8120e-06, 7.5480e-07, 9.5081e-07, 7.2050e-08, 4.3525e-06, 6.8173e-06,\n",
       "        2.2957e-08, 1.1557e-05, 5.2345e-06, 1.5898e-06, 1.8939e-07, 2.2107e-08,\n",
       "        1.3314e-06, 1.3319e-06, 5.1968e-07, 1.3108e-07, 3.5477e-07, 6.0740e-08,\n",
       "        5.9939e-07, 2.1261e-06, 1.6084e-07, 7.0539e-07, 5.0283e-08, 3.5927e-08,\n",
       "        1.2562e-06, 9.5379e-08, 7.1771e-07, 2.5626e-07, 4.2187e-07, 1.5550e-07,\n",
       "        2.7054e-08, 8.4568e-07, 2.3118e-07, 7.7245e-08, 9.6021e-08, 1.5307e-06,\n",
       "        4.0268e-05, 1.2957e-06, 2.0157e-06, 6.2042e-07, 2.3224e-08, 1.2215e-05,\n",
       "        1.8632e-08, 1.2135e-06, 2.5051e-07, 1.7701e-07, 3.8655e-07, 6.1823e-07,\n",
       "        1.2091e-07, 8.6273e-06, 1.9431e-06, 6.1560e-08, 7.0178e-07, 2.5916e-08,\n",
       "        1.0575e-08, 5.9256e-06, 5.4707e-08, 7.5988e-07, 5.6696e-07, 6.2963e-07,\n",
       "        1.1855e-06, 2.1446e-07, 3.9729e-07, 1.8575e-07, 5.0188e-08, 3.1428e-07,\n",
       "        5.0814e-08, 7.9874e-06, 7.2488e-06, 3.1813e-06, 1.2824e-07, 1.2036e-06,\n",
       "        2.0394e-06, 9.0585e-09, 8.0256e-07, 1.3272e-06, 2.2552e-06, 7.3384e-06,\n",
       "        1.5290e-07, 6.3173e-06, 7.7633e-07, 3.1748e-06, 1.6701e-06, 5.2980e-06,\n",
       "        1.5568e-06, 5.2602e-07, 5.6682e-08, 5.8480e-08, 5.7474e-07, 3.1360e-06,\n",
       "        2.4832e-07, 1.5567e-06, 1.4152e-07, 2.1850e-07, 5.3854e-07, 1.4571e-06,\n",
       "        6.2095e-05, 2.2086e-07, 1.3770e-07, 7.9230e-07, 1.7983e-08, 3.4416e-08,\n",
       "        3.5489e-08, 7.6531e-09, 1.2794e-08, 4.9328e-07, 2.4016e-07, 2.3584e-07,\n",
       "        1.0688e-07, 3.9266e-09, 2.2619e-08, 5.6264e-07, 3.9889e-07, 1.3138e-06,\n",
       "        2.6149e-06, 1.1696e-06, 2.1093e-07, 6.3822e-08, 9.9232e-10, 5.7741e-07,\n",
       "        2.8332e-05, 1.2564e-07, 1.9252e-08, 3.7256e-05, 1.6249e-06, 1.6345e-07,\n",
       "        4.5495e-06, 3.1086e-07, 7.3881e-08, 1.1823e-07, 8.0710e-08, 6.0378e-09,\n",
       "        2.9083e-07, 1.2605e-06, 2.7138e-08, 1.0366e-08, 4.0560e-07, 1.8149e-07,\n",
       "        7.3705e-08, 1.0584e-06, 5.9977e-07, 3.3207e-06, 4.8707e-05, 1.3009e-06,\n",
       "        8.1447e-08, 4.1839e-08, 7.6198e-08, 2.8049e-06, 5.0465e-06, 1.5031e-07,\n",
       "        1.4891e-07, 2.2689e-06, 4.8566e-06, 5.1221e-07, 2.1898e-06, 2.4639e-06,\n",
       "        1.2036e-05, 9.8806e-07, 1.3318e-07, 7.2312e-08, 1.0212e-07, 1.2166e-09,\n",
       "        2.8291e-07, 1.3156e-07, 1.0863e-07, 7.3494e-08, 3.1575e-08, 9.9843e-08,\n",
       "        5.6838e-08, 4.3566e-09, 2.6245e-10, 6.0702e-08, 4.7207e-08, 3.8278e-05,\n",
       "        4.0712e-07, 8.1582e-08, 4.9660e-06, 1.9198e-08, 7.2931e-07, 1.7520e-06,\n",
       "        5.8302e-07, 1.2819e-06, 9.5599e-08, 9.4780e-07, 1.7571e-06, 1.1092e-08,\n",
       "        5.8419e-07, 3.5699e-07, 4.9283e-08, 1.4435e-08, 2.4408e-08, 8.3978e-08,\n",
       "        1.0465e-05, 4.2026e-06, 2.9578e-07, 4.5665e-07, 3.1691e-08, 1.5490e-06,\n",
       "        2.0772e-07, 1.7518e-07, 1.1897e-08, 6.5624e-07, 2.7570e-05, 8.4201e-08,\n",
       "        5.8727e-08, 2.5218e-06, 9.1918e-08, 5.5542e-08, 8.7646e-10, 4.0497e-08,\n",
       "        5.1094e-07, 6.5864e-10, 4.7913e-07, 6.7880e-07, 6.7747e-07, 5.6760e-06,\n",
       "        1.8832e-06, 3.5185e-08, 1.3557e-06, 1.9059e-05, 5.8536e-07, 5.8420e-06,\n",
       "        5.1945e-07, 8.1817e-08, 1.4543e-06, 3.8968e-08, 6.8625e-06, 4.5604e-08,\n",
       "        1.5777e-06, 3.0850e-07, 2.2754e-07, 2.9226e-06, 1.8069e-06, 7.3991e-08,\n",
       "        3.1219e-07, 2.3250e-07, 4.6931e-07, 2.9827e-10, 2.3096e-09, 6.0671e-09,\n",
       "        4.9259e-08, 7.3002e-09, 2.3616e-06, 4.2239e-05])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_tensor = torch.from_numpy(final_pre_process)\n",
    "last_input_tensor = input_tensor.to(memory_format=torch.channels_last)\n",
    "input_batch = last_input_tensor.unsqueeze(0) # create a mini-batch as expected by the model\n",
    "with torch.no_grad():\n",
    "    output = test_channels_last_model(input_tensor)\n",
    "\n",
    "torch.nn.functional.softmax(output[0], dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(258)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "values, indices = torch.max(output[0], 0)\n",
    "values\n",
    "indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
